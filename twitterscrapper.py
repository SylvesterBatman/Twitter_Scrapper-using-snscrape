# -*- coding: utf-8 -*-
"""TwitterScrapper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CjacSB-oen30jrwJINLa1JcA6fzeG9BQ
"""

#install snscrape
#!pip install -q snscrape

import snscrape.modules.twitter as sntwitter
import pandas as pd

#Created a list to append all tweet attributes(data)
attributes_container = []

for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:Batman').get_items()):
  if i>100:
    break
  attributes_container.append([tweet.date,tweet.id,tweet.url,tweet.content,tweet.user,tweet.replyCount,tweet.retweetCount,tweet.lang,tweet.source,tweet.likeCount])

tweets_df=pd.DataFrame(attributes_container, columns=["Date","ID","URL","Tweet Content","User","Reply Count","Retweet Count","Language","Source","Like Count"])

a=tweets_df.head(100)

print(a)

a

from pymongo import MongoClient

tweets_df.to_csv("DC.csv")

import json

tweets_df.to_json("DC.json")

py=MongoClient("mongodb://Sylvester_Berry:password@ac-igtjgce-shard-00-00.rj2uwxp.mongodb.net:27017,ac-igtjgce-shard-00-01.rj2uwxp.mongodb.net:27017,ac-igtjgce-shard-00-02.rj2uwxp.mongodb.net:27017/?ssl=true&replicaSet=atlas-2o5uoa-shard-0&authSource=admin&retryWrites=true&w=majority")

f = open("/content/DC.json")
data = json.load(f)
print(data)
print(type(data))

py6=py["Twitter_Scrapper"]
pycollection=py6["Batman"]
pycollection.insert_one(data)

pip install streamlit

import streamlit as st
import pandas as pd
